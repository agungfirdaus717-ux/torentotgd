{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAapMwYCKGZi0LNsfhyK76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agungfirdaus717-ux/torentotgd/blob/main/SubTranslatorNLLB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA3dmHY0ubpC"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SRT Translator (Hugging Face Transformers)\n",
        "# Model: facebook/nllb-200-distilled-600M\n",
        "# ============================================================\n",
        "# ‚úÖ Hasil lebih natural, mirip subtitle resmi\n",
        "# ‚úÖ Tanpa API key\n",
        "# ‚ö†Ô∏è Butuh GPU Colab untuk performa lebih cepat\n",
        "# ============================================================\n",
        "\n",
        "!pip install transformers sentencepiece srt ftfy tqdm\n",
        "\n",
        "import io, os\n",
        "import srt\n",
        "from tqdm import tqdm\n",
        "from ftfy import fix_text\n",
        "from google.colab import files\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# =========================\n",
        "# KONFIGURASI\n",
        "# =========================\n",
        "MODEL_NAME   = \"facebook/nllb-200-distilled-600M\"\n",
        "SOURCE_LANG  = \"eng_Latn\"   # kode bahasa sumber (contoh: English Latin)\n",
        "TARGET_LANG  = \"ind_Latn\"   # kode bahasa target (contoh: Indonesian Latin)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# =========================\n",
        "# LOAD MODEL & TOKENIZER\n",
        "# =========================\n",
        "print(\"üîÑ Download model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "# =========================\n",
        "# UPLOAD FILE\n",
        "# =========================\n",
        "print(\"üìÇ Upload file .srt...\")\n",
        "uploaded = files.upload()\n",
        "in_name = list(uploaded.keys())[0]\n",
        "raw = uploaded[in_name]\n",
        "\n",
        "text_data = fix_text(io.StringIO(raw.decode(\"utf-8\", errors=\"ignore\")).read())\n",
        "subs = list(srt.parse(text_data))\n",
        "print(f\"‚úÖ Loaded {len(subs)} subtitle entries.\")\n",
        "\n",
        "# =========================\n",
        "# TRANSLATION FUNCTION\n",
        "# =========================\n",
        "def translate_batch(texts, src_lang, tgt_lang, batch_size=8, max_length=512):\n",
        "    results = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Menerjemahkan\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
        "        # setting bahasa sumber & target\n",
        "        inputs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=max_length)\n",
        "        out_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        results.extend(out_texts)\n",
        "    return results\n",
        "\n",
        "# =========================\n",
        "# AMBIL TEKS SUBTITLE\n",
        "# =========================\n",
        "original_texts = [s.content.strip().replace(\"\\n\", \" \") for s in subs]\n",
        "\n",
        "# =========================\n",
        "# TRANSLATE\n",
        "# =========================\n",
        "translated_texts = translate_batch(original_texts, SOURCE_LANG, TARGET_LANG)\n",
        "\n",
        "# =========================\n",
        "# SUSUN ULANG SUBTITLE\n",
        "# =========================\n",
        "new_subs = []\n",
        "for orig, trans in zip(subs, translated_texts):\n",
        "    new_item = srt.Subtitle(\n",
        "        index=orig.index,\n",
        "        start=orig.start,\n",
        "        end=orig.end,\n",
        "        content=trans.strip()\n",
        "    )\n",
        "    new_subs.append(new_item)\n",
        "\n",
        "out_text = srt.compose(new_subs)\n",
        "\n",
        "# =========================\n",
        "# SIMPAN HASIL\n",
        "# =========================\n",
        "base, ext = os.path.splitext(in_name)\n",
        "out_name = f\"{base}.id.srt\"\n",
        "with open(out_name, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(out_text)\n",
        "\n",
        "print(f\"‚úÖ Selesai ‚Üí {out_name}\")\n",
        "files.download(out_name)\n"
      ]
    }
  ]
}