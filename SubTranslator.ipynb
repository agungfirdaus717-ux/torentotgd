{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP99mizD7w3m8eKQWwgesaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agungfirdaus717-ux/torentotgd/blob/main/SubTranslator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcX-96hgMC9A"
      },
      "outputs": [],
      "source": [
        "# @title Translate Multiple SRT & ASS to Indonesian (No API Key)\n",
        "# ⚙️ Setup & Install\n",
        "!pip -q install transformers sentencepiece sacremoses langdetect regex tqdm\n",
        "\n",
        "# --- Imports\n",
        "import os, re, io\n",
        "from langdetect import detect\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "# --- GPU check\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', device)\n",
        "\n",
        "# ============================\n",
        "# 🔧 CONFIG\n",
        "# ============================\n",
        "SRC_LANG = 'auto'\n",
        "TGT_LANG = 'id'\n",
        "MODEL_NAME = 'facebook/m2m100_418M'\n",
        "MAX_CHARS_PER_LINE = 200\n",
        "BATCH_SIZE = 16\n",
        "PRESERVE_CASE = True\n",
        "GLOSSARY = {}\n",
        "\n",
        "# ============================\n",
        "# 📂 Upload Banyak file (SRT/ASS)\n",
        "# ============================\n",
        "from google.colab import files\n",
        "uploads = files.upload()\n",
        "input_files = [fn for fn in uploads.keys() if fn.lower().endswith(('.srt','.ass'))]\n",
        "print('Files loaded:', input_files)\n",
        "\n",
        "# ============================\n",
        "# 🧩 Utilities\n",
        "# ============================\n",
        "TIMECODE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*(\\d+)\\s*\\n\n",
        "    (\\d{2}:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d{2}:\\d{2}:\\d{2},\\d{3})\\s*\\n\n",
        "    (.*?)\\n{1,}(?=\\s*\\d+\\s*\\n\\d{2}:|\\Z)\n",
        "\"\"\", re.VERBOSE | re.DOTALL | re.MULTILINE)\n",
        "\n",
        "PROTECT_PATTERNS = [\n",
        "    (re.compile(r\"<[^>]+>\"), 'TAG'),\n",
        "    (re.compile(r\"{[^}]+}\"), 'BRACE'),\n",
        "    (re.compile(r\"\\\\N\"), 'NEWLINE'),\n",
        "    (re.compile(r\"\\u266A|\\u266B|\\u266C|\\u2669|\\u266F|\\u266D|\\u266E|♪\"), 'NOTE'),\n",
        "]\n",
        "\n",
        "PLACEHOLDER_PREFIX = \"[[PH-\"\n",
        "\n",
        "def protect_text(s):\n",
        "    placeholders = []\n",
        "    def repl_factory(label):\n",
        "        def _repl(m):\n",
        "            placeholders.append(m.group(0))\n",
        "            return f\"{PLACEHOLDER_PREFIX}{label}-{len(placeholders)-1}]]\"\n",
        "        return _repl\n",
        "    for pat, label in PROTECT_PATTERNS:\n",
        "        s = re.sub(pat, repl_factory(label), s)\n",
        "    return s, placeholders\n",
        "\n",
        "def restore_text(s, placeholders):\n",
        "    def _repl(m):\n",
        "        i = int(re.findall(r\"-(\\d+)]]$\", m.group(0))[0])\n",
        "        return placeholders[i]\n",
        "    return re.sub(re.escape(PLACEHOLDER_PREFIX) + r\"[A-Z]+-\\d+]]\", _repl, s)\n",
        "\n",
        "def apply_glossary_pre(s):\n",
        "    for k in sorted(GLOSSARY.keys(), key=len, reverse=True):\n",
        "        s = re.sub(rf\"(?i)\\b{re.escape(k)}\\b\", lambda _: f\"[[GLS-{k}]]\", s)\n",
        "    return s\n",
        "\n",
        "def apply_glossary_post(s):\n",
        "    for k, v in GLOSSARY.items():\n",
        "        s = s.replace(f\"[[GLS-{k}]]\", v)\n",
        "    return s\n",
        "\n",
        "def preserve_caps(src, tgt):\n",
        "    if src.strip() and src.upper() == src and not any(c.islower() for c in src):\n",
        "        return tgt.upper()\n",
        "    return tgt\n",
        "\n",
        "def split_long_line(line, max_chars=MAX_CHARS_PER_LINE):\n",
        "    if len(line) <= max_chars:\n",
        "        return [line]\n",
        "    parts = re.split(r\"([.!?])\", line)\n",
        "    chunks, cur = [], ''\n",
        "    for chunk in parts:\n",
        "        if len(cur) + len(chunk) <= max_chars:\n",
        "            cur += chunk\n",
        "        else:\n",
        "            if cur:\n",
        "                chunks.append(cur.strip())\n",
        "            cur = chunk\n",
        "    if cur:\n",
        "        chunks.append(cur.strip())\n",
        "    return [c for c in chunks if c]\n",
        "\n",
        "# ============================\n",
        "# 🚀 Load model\n",
        "# ============================\n",
        "print('Loading model:', MODEL_NAME)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# ============================\n",
        "# 🔄 Proses setiap file (SRT/ASS)\n",
        "# ============================\n",
        "for filepath in input_files:\n",
        "    print(\"\\n=== Processing:\", filepath, \"===\")\n",
        "    base, ext = os.path.splitext(filepath)\n",
        "\n",
        "    with io.open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # ============================\n",
        "    # Jika format SRT\n",
        "    # ============================\n",
        "    if ext.lower() == '.srt':\n",
        "        content = content.strip() + '\\n\\n'\n",
        "        blocks = []\n",
        "        for m in re.finditer(TIMECODE_RE, content):\n",
        "            idx = int(m.group(1))\n",
        "            start, end = m.group(2), m.group(3)\n",
        "            text = m.group(4).strip('\\n')\n",
        "            lines = text.split('\\n')\n",
        "            norm_lines = []\n",
        "            for ln in lines:\n",
        "                norm_lines.extend(split_long_line(ln))\n",
        "            blocks.append({'idx': idx, 'start': start, 'end': end, 'lines': norm_lines})\n",
        "\n",
        "        joined_sample = '\\n'.join(['\\n'.join(b['lines']) for b in blocks[:50]])\n",
        "        src_lang = SRC_LANG\n",
        "        if SRC_LANG == 'auto':\n",
        "            try:\n",
        "                src_lang = detect(joined_sample)\n",
        "            except Exception:\n",
        "                src_lang = 'en'\n",
        "        print('Source lang:', src_lang, '-> Target:', TGT_LANG)\n",
        "\n",
        "        try:\n",
        "            tokenizer.src_lang = src_lang\n",
        "        except Exception:\n",
        "            tokenizer.src_lang = 'en'\n",
        "        forced_id = tokenizer.get_lang_id(TGT_LANG)\n",
        "\n",
        "        source_lines, meta = [], []\n",
        "        for bi, b in enumerate(blocks):\n",
        "            for li, line in enumerate(b['lines']):\n",
        "                orig = line\n",
        "                line = apply_glossary_pre(line)\n",
        "                prot, ph = protect_text(line)\n",
        "                source_lines.append(prot)\n",
        "                meta.append((bi, li, orig, ph))\n",
        "\n",
        "        translated_lines = [''] * len(source_lines)\n",
        "        for i in tqdm(range(0, len(source_lines), BATCH_SIZE)):\n",
        "            batch = source_lines[i:i+BATCH_SIZE]\n",
        "            enc = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                gen = model.generate(**enc, forced_bos_token_id=forced_id, max_length=256)\n",
        "            outs = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "            for j, out in enumerate(outs):\n",
        "                translated_lines[i+j] = out\n",
        "\n",
        "        for k, (bi, li, orig, ph) in enumerate(meta):\n",
        "            text = translated_lines[k]\n",
        "            text = restore_text(text, ph)\n",
        "            text = apply_glossary_post(text)\n",
        "            if PRESERVE_CASE:\n",
        "                text = preserve_caps(orig, text)\n",
        "            blocks[bi]['lines'][li] = text\n",
        "\n",
        "        out_path = f\"{base}.id.srt\"\n",
        "        with io.open(out_path, 'w', encoding='utf-8', errors='ignore') as f:\n",
        "            for b in blocks:\n",
        "                f.write(str(b['idx']) + '\\n')\n",
        "                f.write(f\"{b['start']} --> {b['end']}\\n\")\n",
        "                f.write('\\n'.join(b['lines']).strip() + '\\n\\n')\n",
        "\n",
        "        print('Saved:', out_path)\n",
        "\n",
        "    # ============================\n",
        "    # Jika format ASS\n",
        "    # ============================\n",
        "    elif ext.lower() == '.ass':\n",
        "        lines = content.splitlines()\n",
        "        out_lines = []\n",
        "\n",
        "        # deteksi bahasa dari beberapa dialog awal\n",
        "        sample_dialogues = []\n",
        "        for l in lines:\n",
        "            if l.strip().startswith(\"Dialogue:\"):\n",
        "                parts = l.split(\",\", 9)\n",
        "                if len(parts) >= 10:\n",
        "                    sample_dialogues.append(parts[9])\n",
        "                if len(sample_dialogues) >= 20:\n",
        "                    break\n",
        "        joined_sample = \" \".join(sample_dialogues)\n",
        "        src_lang = SRC_LANG\n",
        "        if SRC_LANG == 'auto':\n",
        "            try:\n",
        "                src_lang = detect(joined_sample)\n",
        "            except Exception:\n",
        "                src_lang = 'en'\n",
        "        print('Source lang:', src_lang, '-> Target:', TGT_LANG)\n",
        "\n",
        "        try:\n",
        "            tokenizer.src_lang = src_lang\n",
        "        except Exception:\n",
        "            tokenizer.src_lang = 'en'\n",
        "        forced_id = tokenizer.get_lang_id(TGT_LANG)\n",
        "\n",
        "        # siapkan teks\n",
        "        source_lines, meta = [], []\n",
        "        for idx, l in enumerate(lines):\n",
        "            if l.strip().startswith(\"Dialogue:\"):\n",
        "                parts = l.split(\",\", 9)\n",
        "                if len(parts) >= 10:\n",
        "                    orig_text = parts[9]\n",
        "                    text = apply_glossary_pre(orig_text)\n",
        "                    prot, ph = protect_text(text)\n",
        "                    source_lines.append(prot)\n",
        "                    meta.append((idx, orig_text, ph))\n",
        "\n",
        "        translated_lines = [''] * len(source_lines)\n",
        "        for i in tqdm(range(0, len(source_lines), BATCH_SIZE)):\n",
        "            batch = source_lines[i:i+BATCH_SIZE]\n",
        "            enc = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                gen = model.generate(**enc, forced_bos_token_id=forced_id, max_length=512)\n",
        "            outs = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "            for j, out in enumerate(outs):\n",
        "                translated_lines[i+j] = out\n",
        "\n",
        "        for k, (idx, orig_text, ph) in enumerate(meta):\n",
        "            text = translated_lines[k]\n",
        "            text = restore_text(text, ph)\n",
        "            text = apply_glossary_post(text)\n",
        "            if PRESERVE_CASE:\n",
        "                text = preserve_caps(orig_text, text)\n",
        "            parts = lines[idx].split(\",\", 9)\n",
        "            parts[9] = text\n",
        "            lines[idx] = \",\".join(parts)\n",
        "\n",
        "        out_path = f\"{base}.id.ass\"\n",
        "        with io.open(out_path, 'w', encoding='utf-8', errors='ignore') as f:\n",
        "            f.write(\"\\n\".join(lines))\n",
        "\n",
        "        print('Saved:', out_path)\n",
        "\n",
        "# ============================\n",
        "# ⬇️ Download semua hasil\n",
        "# ============================\n",
        "from google.colab import files\n",
        "for fn in os.listdir():\n",
        "    if fn.endswith('.id.srt') or fn.endswith('.id.ass'):\n",
        "        files.download(fn)\n"
      ]
    }
  ]
}